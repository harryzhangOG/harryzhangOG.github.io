<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Harry Zhang</title>
  
  <meta name="author" content="Harry Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cmu-seal.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Harry H. Zhang</name>
              </p>
              <p>
                I am a PhD student in the <a href="https://web.mit.edu/sparklab/">SPARK Lab</a> of <a href="https://lids.mit.edu/">MIT LIDS</a>. I am extremely fortunate to be advised by <a href="https://lucacarlone.mit.edu/">Prof. Luca Carlone</a>.
              <p>Prior to MIT, I was a MS-Research student in the <a href="https://ri.cmu.edu/">CMU Robotics Institute</a> studying Artificial Intelligence and Robotics, advised by Prof. <a href="https://davheld.github.io/">David Held</a>.  
              </p>
              <p>
              Prior to CMU, I earned my B.S. (2017-2021) with Honors from UC Berkeley with a major in <a href="https://eecs.berkeley.edu/">EECS</a> and a minor in <a href="https://me.berkeley.edu/">Mechanical Engineering</a>. During my time at Berkeley, I did research under <a href="https://goldberg.berkeley.edu/">Prof. Ken Goldberg</a> and <a href="https://ichnow.ski/">Dr. Jeffrey Ichnowski</a> in <a href="http://autolab.berkeley.edu/">AUTOLab</a>. I maintain and curate a popular deep
              reinforcement learning tutorial on <a href="https://github.com/harryzhangOG/Deep-RL-Notes">my Github</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:harryz@mit.edu">Email</a> &nbsp/&nbsp
                <a href="data/resume_v2_hz.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/harry-z-292616148/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=e-p7KiUAAAAJ&hl=en&authuser=4">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/harryzhangOG/">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/harryzhangOG/">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/harry-zhang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/harry-zhang.png" class="hoverZoomLink"></a>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News and Updates</heading>
              <p>In reverse chronological order:</p>
              <ul>
                 <li>
                      <strong>Sep 2022:</strong> TAX-Pose accepted to CoRL, see you in New Zealand!
                 <li>
                      <strong>May 2022:</strong> I am joining Amazon this summer as an applied research scientist, working on 3D learning problems.
                 <li>
                      <strong>Apr. 2022:</strong> FlowBot3D accepted to RSS, see you in NYC!
                 <li>
                      <strong>Jan. 2022:</strong> We submitted the FlowBot3D paper to RSS.
                  <li>
                      <strong>Aug. 2021:</strong> Started grad school at CMU RI. Looking forward to Pittsburgh, PA.
                  <li>
                      <strong>May. 2021:</strong> Won the <a href="https://www2.eecs.berkeley.edu/Students/Awards/4/#:~:text=Warren%20Y.-,Dere%20Design%20Award,an%20upper%20division%20design%20course.">Warren Y. Dere Award</a> from UC Berkeley EECS.
                  <li>
                      <strong>Mar. 2021:</strong> Dynamic cable manipulation paper accepted to ICRA 2021.
                  <li>
                      <strong>Nov. 2020:</strong> Dynamic cable manipulation paper featured at Bay Area Robotics Symposium (BARS) hosted by Stanford.
                  <li>
                      <strong>Nov. 2020:</strong> Dynamic cable manipulation paper submitted to ICRA 2021.
                  <li> 
                      <strong>Jun. 2020:</strong> Dex-Net AR got featured on <a href="https://venturebeat.com/2020/06/11/dex-net-ar-uses-apples-arkit-to-train-robots-to-grasp-objects/">VentureBeat</a>. 
                  <li> 
                      <strong>Jun. 2020:</strong> Dex-Net AR got featured on <a href="https://www.sohu.com/a/401322985_213766?_f=index_pagefocus_7&_trans_=000014_bdss_dkbjxgyq">Sohu</a>.  
                  <li> 
                      <strong>Aug. 2019:</strong> AI4All Berkeley was a blast. We organized an AI crash course for under-represented HS/MS students. See summary <a href="http://bcnm.berkeley.edu/news-research/3356/ken-goldberg-recycling-with-robots">here</a>.  
              <ul>

              </p>
            </td>
          </tr>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
              My current research focuses on practical problems that artificial intelligence faces in real life. My interests are on the intersection of robotics, computer vision, 
              reinforcement learning, and control theory. I would like to let AI agents gain a better understanding of the structure of the world in terms of perception, modeling, and manipulation. 
              Specifically, I think about how robots can perceive the world in a way that facilitates downstream policy and planning. I firmly believe cleverly-designed learned representations of both visual input and action output
             could significantly improve downstream policy learning in robotic manipulation tasks.
              <a href="http://stanford.edu/~lmackey/">Quixotic though it may sound,</a> I hope to use AI and robotics to change the world for the better. Here is the <a href="data/SOP2022.pdf">Personal Statement</a> I used for my PhD applications.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Peer-Reviewed Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/taxpose.gif">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/tax-pose/home">
                <papertitle>TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation</papertitle>
              </a>
              <br>
              <a href="https://www.ri.cmu.edu/ri-people/brian-e-okorn/">Brian Okorn*</a>,
              Chu Er Pan*,
              <strong>Harry Zhang*</strong>,
              <a href="https://beisner.me/">Benjamin Eisner*</a>,
              <a href="https://davheld.github.io/">David Held</a>
              <br>
              <em>Accepted to Conference on Robot Learning (CoRL), 2022 (* indicates equal contribution) </em>
              <br>
              <a href="https://arxiv.org/abs/2211.09325">Arxiv</a> |
              <a href="">Code</a> | 
              <a href="">Video</a> |
              <a href="https://openreview.net/forum?id=YmJi0bTfeNX">Open Review</a>
              <br>
              <p></p>
              <p>We conjecture that the task-specific pose relationship between relevant parts of interacting  objects is a generalizable notion of a manipulation task that can transfer to new objects. We call this task-specific pose relationship "cross-pose". We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/montage.gif">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/articulated-flowbot-3d/home">
                <papertitle>FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated Objects</papertitle>
              </a>
              <br>
              <a href="https://beisner.me/">Benjamin Eisner*</a>,
              <strong>Harry Zhang*</strong>,
              <a href="https://davheld.github.io/">David Held</a>
              <br>
              <em>Accepted to Robotics Science and Systems (RSS), 2022 (* indicates equal contribution) - <a style="color:red;">Long talk, Best Paper Award Finalist (Selection Rate 1.5%)</a>.</em>
              <br>
              <a href="http://arxiv.org/abs/2205.04382">Arxiv</a> |
              <a href="">Code</a> | 
              <a href="">Video</a> |
              <a href="https://youtu.be/AZjzndMv9dU?t=2277">Berkeley CPAR Talk</a> |
              <a href="https://www.mittrchina.com/news/detail/10606">MIT Technology Review China</a> | 
              <a href="https://www.sohu.com/a/554596988_129720">Synced Review Sohu</a>
              <a href="https://www.cs.cmu.edu/news/2022/robots-articulated-objects">CMU Research Highlights</a>
              <br>
              <p></p>
              <p>We explore a novel method to perceive and manipulate 3D articulated objects that generalizes to enable the robot to articulate unseen classes of objects.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/avplug.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>AVPLUG: Approach Vector Planning for Unicontact Grasping amid Clutter</papertitle>
              </a>
              <br>
              Yahav Avigal*,
              Vishal Satish*, 
              <strong>Harry Zhang</strong>,
              Huang Huang, 
              <a href="https://mjd3.github.io/">Michael Danielczuk</a>, 
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>,
              <a href="goldberg.berkeley.edu/">Ken Goldberg</a>
              <br>
              <em>Accepted to Conference on Automation Science and Engineering (CASE), 2021.</em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/9551652">Arxiv</a> |
              <a href="">Code</a> | 
              <a href="">Video</a>
              <br>
              <p></p>
              <p>We present present AVPLUG: Approach Vector PLanning for Unicontact Grasping: an algorithm for efficiently finding the approach vector using an efficient oct-tree occupancy model and Minkowski sum computation to maximize information gain.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/20201030_Goldberg_AVL_0623_stack.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/berkeley.edu/dynrope/home">
                <papertitle>Robots of the Lost Arc: Self-Supervised Learning to Dynamically Manipulate Fixed-Endpoint Cables</papertitle>
              </a>
              <br>
              
              <strong>Harry Zhang</strong>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>,
              <a href="https://people.eecs.berkeley.edu/~seita/">Daniel Seita</a>,
              Jonathan Wang,
              Huang Huang,
              <a href="goldberg.berkeley.edu/">Ken Goldberg</a>
              <br>
              <em>Accepted to International Conference on Robotics and Automation (ICRA)</em>, 2021  
              <br>
              <a href="https://arxiv.org/abs/2011.04840">Arxiv</a> |
              <a href="https://github.com/harryzhangOG/rope">Code</a> | 
              <a href="https://youtu.be/MsROcNm4qgs?t=2318">Bay Area Robotics Symposium Coverage</a> |  
              <a href="https://youtu.be/R8DqX5Hg_IE?t=193">ICRA 2022 Deformable Object Manipulation Workshop</a> 
              <p></p>
              <p> We propose a self-supervised learning framework that enables a UR5 robot to perform these three tasks. The framework finds a 3D apex point for the robot arm, which, together with a task-specific trajectory function, defines an arcing motion that dynamically manipulates the cable to perform tasks with varying obstacle and target locations. 
              </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/dex-net-ar.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/berkeley.edu/dex-net-ar/home">
                <papertitle>Dex-Net AR: Distributed Deep Grasp Planning Using a Commodity Cellphone and Augmented Reality App</papertitle>
              </a>
              <br>
              <strong>Harry Zhang</strong>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>,
              Yahav Avigal, 
              <a href="https://people.eecs.berkeley.edu/~jegonzal/">Joseph Gonzalez</a>, 
              <a href="https://people.eecs.berkeley.edu/~istoica/">Ion Stoica</a>, 
              <a href="goldberg.berkeley.edu/">Ken Goldberg</a>

              <br>
              <em>Accepted to International Conference on Robotics and Automation (ICRA)</em>, 2020  
              <br>
              <a href="https://goldberg.berkeley.edu/pubs/2020-ICRA-Zhang-Dex-Net-AR.pdf">Arxiv</a> |
              <a href="https://github.com/harryzhangOG/dexnet-ar">Code</a> | 
              <a href="https://youtu.be/piOTIFZgKME">Video</a> |  
              <a href="https://venturebeat.com/2020/06/11/dex-net-ar-uses-apples-arkit-to-train-robots-to-grasp-objects/">VentureBeat Coverage</a> |  
              <a href="https://www.sohu.com/a/401322985_213766?_f=index_pagefocus_7&_trans_=000014_bdss_dkbjxgyq">Sohu Coverage (in Mandarin)</a>  
              <br>
              <p></p>
              <p>We present a distributed pipeline, Dex-Net AR, that allows point clouds to be uploaded to a server in our lab, cleaned, and evaluated by Dex-Net grasp planner to generate a grasp axis that is returned and displayed as an overlay on the object. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/orient.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://berkeleyautomation.github.io/Orienting_Novel_3D_Objects/">
                <papertitle>Orienting Novel Objects using Self-Supervised Rotation Estimation</papertitle>
              </a>
              <br>
              Shivin Devgon,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>,
              <a href="https://abalakrishna123.github.io/">Ashwin Balakrishna</a>,
              <strong>Harry Zhang</strong>,
              <a href="goldberg.berkeley.edu/">Ken Goldberg</a>
              <br>
              <em>Accepted to Conference on Automation Science and Enigeering (CASE), 2020.</em>
              <br>
              <a href="https://drive.google.com/file/d/1BzpqTT801NK8AmrULfJJOQsquN0vqnzx/view">Arxiv</a> |
              <a href="https://github.com/BerkeleyAutomation/Orienting_Novel_3D_Objects">Code</a> | 
              <a href="https://www.youtube.com/watch?v=nij_JgNP1qw&ab_channel=ShivinDevgon">Video</a>
              <br>
              <p></p>
              <p>We present an algorithm to orient novel objects given a depth image of the object in its current and desired orientation.</p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/dynamic-cable-160.gif">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Self-Supervised Learning of Dynamic Planar Manipulation of Free-End Cables</papertitle>
              </a>
              <br>
              Jonathan Wang*,
              Huang Huang*, 
              Vincent Lim, 
              <strong>Harry Zhang</strong>,
              <a href="https://ichnow.ski/">Jeffrey Ichnowski</a>,
              <a href="https://people.eecs.berkeley.edu/~seita/">Daniel Seita</a>,
              Yunliang Chen, 
              <a href="goldberg.berkeley.edu/">Ken Goldberg</a>
              <br>
              <em>Preprint, in submission to International Conference on Robotics and Automation (ICRA), 2022.</em>
              <br>
              <a href="">Arxiv</a> |
              <a href="">Code</a> | 
              <a href="">Video</a>
              <br>
              <p></p>
              <p>We present an algorithm to train a robot to control free-end cables in a self-supervised fashion.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/salved.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Safe Deep Model-Based Reinforcement Learning with Lyapunov Functions</papertitle>
              </a>
              <br>
              Bobby Yan*, 
              <strong>Harry Zhang*</strong>,
              Huang Huang*, 
              <br>
              <em>Preprint, 2022.</em>
              <br>
              <a href="">Arxiv</a> |
              <a href="">Code</a> | 
              <a href="">Video</a>
              <br>
              <p></p>
              <p>We introduce andexplore a novel method for adding safety constraints for model-based RL during training and policy learning.</p>
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/cmu.png">
            </td>
            <td width="75%" valign="center">
              <p>
                <b>10-725: Graduate Convex Optimization</b><br>
                <b>16-385: Computer Vision</b> <br>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/berkeley_eecs.png">
            </td>
            <td width="75%" valign="center">
              <p>
                <b>CS 189: Introduction to Machine Learning</b> <br>
              </p>
              <p>
                <b>EE 127: Introduction to Convex Optimization</b> <br>
              </p>
                <b>CS 188: Introduction to Artificial Intelligence</b> <br>
              </p>
                <b>CS 170: Algorithms</b> <br>
              </p>
                <b>ME C231A: Model Predictive Control</b> <br>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Website template from Jon Barron</a>
                <a href="https://clustrmaps.com/site/1bq7z"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=GiGzh5Add_ZbUUGpnWqADykY66Y_HPEziDZJuH-aD1Q&cl=ffffff" /></a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>


</html>
